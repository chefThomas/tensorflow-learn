# Chapter 1: What is AI

## What is Intelligence?

- intelligence was outside the realm of mechanical structures until advent of computers.
- 1950s: AI term coined
- `AI`: imitation of human activities that we believe requires intelligence, regardless of sophistication. Even losing tic-tac-toe algorithm can be called AI

## History of AI

- Previously AI at a high-level was a collection of if-then statements
- Machine Learning (ML): an application trained according to data fed to it
- `von Neuman bottleneck`
  - limitation on data flow from memory to CPU imposed by traditional computer architecture.
  - 2000s: overcome by graphics processing units (GPU)
- ML jumped from academia to reality with one of the first ML milestones: `image recognition`, specifically, reading handwritten digits.
  - Hinton used ML` deep neural networks`

## `Neural Networks`

- digital `nodes` (aka `perceptron networks`) simulate neurons
  - activate like synapses
- Vastly layered connectioned of nodes --> `deep learning`
- AI is the super set
  - Machine Learning
    - Deep learning
- Training `iterations` are used to build out neurons based on data --> predictive power increases i.e. the model learns
- We learn through electrical signals induced by stimuli. In ML, the analog is `tensor`.

## Today's AI

- 2015, AI begins outperforming humans in some tasks
  - lawyering
  - cancer detection from x-ray films
- Applications:
  - music, writing, visual content
  - recommending content
  - deduce laws from data
  - visual classification and identifiers

## Why TensorFlow?

- Supported by Google
- supports optimized and tested GPU-boosted code
- online ready
  - can run Tensorflow directly from the browser importing libraries from Google
- offline ready
  - can save the code to a device like a PWA so apps can run without an internet connection
  - speed and cost advantage

| Browser       | Server | Mobile            | Desktop       | IoT          |
| ------------- | ------ | ----------------- | ------------- | ------------ |
| Most browsers | Node   | React Native, PWA | Electron, PWA | Raspberry Pi |

## Types of Machine Learning

### Supervised

- most common form of ML
- uses an 'answer key' of labeled data to train our machine
- Examples
  - image categorization
  - natural language processing (NLP)
  - Image segmentation

### Unsupervised

- Focus on what a machine can learn and report from unlabeled data
- E.g., how many groups of something exist, like plant species in a garden

### Semisupervised

- at least some of the data are labeled
- Generative Adversarial Networks (GAN) like Dall-E
  - trained on examples using semisupervised methods and output is a new example.

### Reinforcement

- Providing a reward for actions when they produce desirable outcomes reinforces the 'correct' actions.
- used in Mario speed runs

## What do Frameworks Provide

- in ML, the models write the algorithms, not the programmer. The programmer writes the trainer with the help of a framework. The algorithm + tuning = model is the output
  - outline parameters
  - define structure
  - location of training data
  - run the training program
    - output is an algorithm that balances the structure defined by the programmer. Algorithms are essentially a collection of numbers associated with a graph, which we collectively call a model.
- We use TensorFlow.js to specify model architecture, load data, and tune the model to improve predictive power.

## What Is a Model and How Do Neural Networks Learn?

- Neural network is an encoded graph with intelligently chosen, randomized values for each neuron
- The first prediction made is as far away from correct as pure random chance, because it hasn't been trained
- After iterating (learning), neural network weights are evaluated and adjusted
- After training, the model can be used to make predictions by feeding it data. A probabilistic results is the output.

<br>
<br>

# Chapter 2. Introducing TensorFlow.js

## Intro to TS

- TS is Google's first public ML framework
- Subsequent variant TS Lite was created for things with less computing power, such as mobile devices and IoT.
- TF.js was the first time the 'Python chains' were lifted from ML.
  - allowed Node servers and browsers to run TS

## Getting Set Up with TS.js in the Browser

- Can import two ways
  - Node Package Manager

```js
npm i @tensorflow/tfjs
```

- Script tag in HTML with the code dl'd local to the HTML file or retrieved from CDN

```html
<script src="https://cdn.jsdeliver.net/npm/@tensorflow/tfjs@2.7.0/dist/tf.min.js"></script>
```

- Then import in JS file with

```js
import * as tf from '@tensorflow/tfjs';
```

\*\* Created a UI for the Toxicity language classifier hosted here: https://dev.dabo6oebgje2x.amplifyapp.com/

<br>
<br>

# Chapter 3. Introducing Tensors

## Why use Tensors and What are They?

- A numeric data type
- Math def: a structured set of values of any dimension, so a JS array is technically a tensor.
- Uniform and unopinionated numeric containers
- Scale quickly
- Work directly with calculation optimizations in GPU or Web Assembly
- A grouping of data ready for calculation
- Tensor and JS Array syntax similar
- Dimensions are called 'ranks'

```js
// Example of rank-two/2d array
const xyCoord = [
  [2, 4],
  [6, 3],
  [7, 1],
];
```

Note: nesting arrays allows us to format data in a way that correlated information can map to one another.

## Creating Tensors

```js
import * as tf from '@tensorflow/tfjs-node';

/*
tensor() - used to create tensors using 1, 2, or 3 args
@param {(number[]|boolean[])} data
@param {integer[]} shape - number rows x number columns
@param {'int32'|'float32'|'boolean'} type - homogenizes/enforces one of three allowed datatypes
*/

// the numeric data we want to convert
const nums = [3, 5, 2, 7, 4];

// first way with tensor()
const tensor1 = tf.tensor(nums);

// second way with tensor#d() provides some additional runtime checking- throws error if a non-1d array is passed
const tensor2 = tf.tensor1d(nums);

// Note: all values must be same type and must be Float32, Int32, bool, complex64, or String

// Enforce float32 typing with third arg
const tensor3 = tf.tensor([1, 4, 2], null, 'float32'); // converts to 1.0, 4.0, 2.0

// Enforce int32
const tensor4 = tf. tensor([3. 2. 7], null, 'int32') // converts to 3, 2, 7
const tensor5 = tf.tensor( [ true, false ], null, 'int32' ) // converts to 1, 0

// Inferred typing (bool)
 const tensor6 = tf.tensor([false, true, true])

// What happens here?
const tensor7 = tf.tensor([1, 3.587, false])  // all converted to Float32. 1, 3.587, 0

/* Specifying shape with the 2nd parameter

*/
const tensor8 = tf.tensor([1, 1, 7, 2, 1, 3, 7, 3, 4], [3, 3])
// above equivalent to
const tensor9 = tf.tensor2d([[1, 1, 7], [2, 1, 3], [7, 3, 4]])

// another example...
const b = tf.tensor( [ 1, 2, 3, 4, 5, 6 ], [ 2, 3 ] ) // 2 rows, 3 cols [[1, 2, 3], [4, 5, 6]]

```

### Tensor properties

Size, data type, and rank

```js
// create 1d tensor and destructure properties
const { size, dtype, rank } = tf.tensor1d([1, 4, 3]);

console.log(size, dtype, rank); // 3 float32 1
```

### Tensor speed

- very fast matrix and vector calculations
- calculations performed in mass groupings
- batch processing

### Tensors and memory mgmt

- the cost to speed benefit is loss of automatic garbage detection and collection
- need to deallocate memory even after variable is orphaned
- tnsors can be live or disposed. If live and the JS variable originally assigned to it as has been reassigned (ie has no reference), then there is a memory leak. If the JS variable persists but tensor is disposed, then error is thrown.
- The proper way to destroy a tensor is to dispose of the tensor and do not attempt to access it

```js
/*
tf.memory() returns object with info on active tensors
*/
let myTensor = tf.tensor([1, 2, 2, 1], [2, 2]);

console.log(tf.memory()); // { unreliable: true, numTensors: 1, numDataBuffers: 1, numBytes: 16 }

myTensor = null;

console.log(tf.memory()); // same as above: { unreliable: true, numTensors: 1, numDataBuffers: 1, numBytes: 16 }. The tensor is still in memory
```

#### Auto cleanup with tidy

Tensors encapsulated in `tf.tidy()` are automatically cleaned up unless flagged to be kept with `tf.keep()` or returned

```js
import * as tf from '@tensorflow/tfjs-node';

let keepOne;
let keepTwo;
let tidyUp;

tf.tidy(() => {
  keepOne = tf.tensor([1, 2, 3]);
  keepTwo = tf.tensor2d([[1], [3]]);
  tidyUp = tf.tensor([1, 3, 7, 2, 1, 1], [2, 3]);

  tf.keep(keepone);

  return keepTwo;
});

// check memory for tensors
console.log(tf.memory().numTensors); // 2

// then dispose of the two explicitly with dispose
keepOne.dispose();
keepTwo.dispose();

// check again
console.log(tf.memory().numTensors); // 0
```

#### Mixing JS and tensors

TF is smart enough to find tensors in JS arrays, so we can use JS loops to create a JS array of tensors and still use `dispose() `to clean up

```js
let tensorArray = [];
for (let i = 0; i <= 4; i++) {
  tensorArray.push(tf.tensor([i, i, i]));
}

console.log(tf.memory().numTensors); // 5

tf.dispose(tensorArray);

console.log(tf.memory().numTensors); // 0
```

### Retrieving Tensor Data (tensor to JS arrays and back)

#### `<tensor>.print()`

#### `<tensor>.array()`

#### `<tensor>.arraySync()`

#### `<tensor>.data()`

#### `<tensor>.dataSync()`

- trying to print a tensor just prints the object with structure, but not the values.
- `<tensor>.print()` to view the data
-

```js
const a = tf.tensor([1, 2, 3]);
const b = tf.tensor([
  [4, 5],
  [3, 1],
]);

a.print(); // Tensor [1, 2, 3]
b.print(); // Tensor [[4, 5], [3, 1]]

console.log(a); // the tensor object with its 9 properties
console.log(a.array()); // Promise { <pending> }
const arrFromTensor = a.array().then;

// sync returns array from tensor
console.log(a.arraySync()); // [1, 2, 3]
const secondElement = a.arraySync()[1];
console.log(secondElement); // 2

// data() returns flattened to 1d Array
const flat = b.dataSync();
console.log(flat); // Float32Array(4) [ 4, 5, 3, 1 ]
```

### Tensor Math, pg. 60

#### Multiplying matrices

- a usecase for this operation is quantifying similarities between two tensors in building recommendation engines. The higher the dot product, the more similar the tensors

```js
const mat1 = [
  [91, 23, 12],
  [65, 23, 12],
  [89, 21, 11],
];

const mat2 = [
  [1, 3, 2],
  [23, 21, 67],
  [90, 43, 61],
];

const dotProduct = tf.matMul(mat1, mat2);

dotProduct.print();
/*
Tensor
    [[1700, 1272, 2455],
     [1674, 1194, 2403],
     [1562, 1181, 2256]]
*/
```
